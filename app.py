import streamlit as st
import PyPDF2
import os
import tempfile
from sentence_transformers import SentenceTransformer
import chromadb
from chromadb.config import Settings
import google.generativeai as genai
import json
import re
from typing import List, Dict, Tuple
import numpy as np

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="PDF ë¬¸ì„œ ë¶„ì„ AI",
    page_icon="ğŸ“„",
    layout="wide"
)

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if 'vectordb' not in st.session_state:
    st.session_state.vectordb = None
if 'collection' not in st.session_state:
    st.session_state.collection = None
if 'embedding_model' not in st.session_state:
    st.session_state.embedding_model = None
if 'documents_loaded' not in st.session_state:
    st.session_state.documents_loaded = False
if 'page_mapping' not in st.session_state:
    st.session_state.page_mapping = {}

class PDFAnalyzer:
    def __init__(self):
        self.embedding_model = None
        self.vectordb = None
        self.collection = None
        self.page_mapping = {}
        
    def initialize_models(self):
        """ëª¨ë¸ ì´ˆê¸°í™”"""
        if self.embedding_model is None:
            with st.spinner("ì„ë² ë”© ëª¨ë¸ì„ ë¡œë”©ì¤‘ì…ë‹ˆë‹¤..."):
                self.embedding_model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')
        
        if self.vectordb is None:
            self.vectordb = chromadb.Client(Settings(
                persist_directory="./chroma_db",
                anonymized_telemetry=False
            ))
            
        return True
    
    def extract_text_from_pdf(self, pdf_file):
        """PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (í˜ì´ì§€ë³„ë¡œ ë¶„ë¦¬)"""
        pages_text = []
        try:
            # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
            with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as tmp_file:
                tmp_file.write(pdf_file.read())
                tmp_file_path = tmp_file.name
            
            # PDF ì½ê¸°
            with open(tmp_file_path, 'rb') as file:
                pdf_reader = PyPDF2.PdfReader(file)
                for page_num, page in enumerate(pdf_reader.pages):
                    page_text = page.extract_text()
                    if page_text.strip():
                        pages_text.append({
                            'page_num': page_num + 1,
                            'text': page_text.strip()
                        })
            
            # ì„ì‹œ íŒŒì¼ ì‚­ì œ
            os.unlink(tmp_file_path)
            
        except Exception as e:
            st.error(f"PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return []
        
        return pages_text
    
    def smart_text_chunking(self, pages_text: List[Dict], chunk_size: int = 1200, overlap: int = 200):
        """ìŠ¤ë§ˆíŠ¸ í…ìŠ¤íŠ¸ ì²­í‚¹ - ë¬¸ë§¥ì„ ê³ ë ¤í•œ ë¶„í• """
        chunks = []
        
        for page_data in pages_text:
            page_num = page_data['page_num']
            text = page_data['text']
            
            # ë¬¸ë‹¨ ë‹¨ìœ„ë¡œ ë¨¼ì € ë¶„í• 
            paragraphs = re.split(r'\n\s*\n', text)
            
            current_chunk = ""
            current_size = 0
            
            for para in paragraphs:
                para = para.strip()
                if not para:
                    continue
                
                # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ì„¸ë¶„í™”
                sentences = re.split(r'(?<=[.!?])\s+', para)
                
                for sentence in sentences:
                    sentence = sentence.strip()
                    if not sentence:
                        continue
                    
                    sentence_size = len(sentence)
                    
                    # ì²­í¬ í¬ê¸° ì´ˆê³¼ ì‹œ ìƒˆ ì²­í¬ ìƒì„±
                    if current_size + sentence_size > chunk_size and current_chunk:
                        chunks.append({
                            'text': current_chunk.strip(),
                            'page_num': page_num,
                            'chunk_id': len(chunks)
                        })
                        
                        # ì˜¤ë²„ë©ì„ ìœ„í•œ ì»¨í…ìŠ¤íŠ¸ ìœ ì§€
                        overlap_sentences = current_chunk.split('.')[-3:]  # ë§ˆì§€ë§‰ 3ë¬¸ì¥ ìœ ì§€
                        current_chunk = '. '.join(overlap_sentences).strip() + '. ' + sentence
                        current_size = len(current_chunk)
                    else:
                        current_chunk += ' ' + sentence
                        current_size += sentence_size + 1
            
            # ë§ˆì§€ë§‰ ì²­í¬ ì¶”ê°€
            if current_chunk.strip():
                chunks.append({
                    'text': current_chunk.strip(),
                    'page_num': page_num,
                    'chunk_id': len(chunks)
                })
        
        return chunks
    
    def create_vector_database(self, chunks: List[Dict], filename: str):
        """í–¥ìƒëœ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ìƒì„±"""
        try:
            # ê¸°ì¡´ ì»¬ë ‰ì…˜ ì‚­ì œ
            try:
                self.vectordb.delete_collection(name="documents")
            except:
                pass
            
            # ìƒˆ ì»¬ë ‰ì…˜ ìƒì„±
            self.collection = self.vectordb.create_collection(
                name="documents",
                metadata={"hnsw:space": "cosine"}
            )
            
            # í˜ì´ì§€ ë§¤í•‘ ì´ˆê¸°í™”
            self.page_mapping = {}
            
            # ì²­í¬ë“¤ì„ ì„ë² ë”©í•˜ê³  ì €ì¥
            with st.spinner(f"ë¬¸ì„œë¥¼ ë²¡í„°í™”í•˜ê³  ìˆìŠµë‹ˆë‹¤... ({len(chunks)}ê°œ ì²­í¬)"):
                progress_bar = st.progress(0)
                
                for i, chunk_data in enumerate(chunks):
                    chunk_text = chunk_data['text']
                    page_num = chunk_data['page_num']
                    chunk_id = chunk_data['chunk_id']
                    
                    # ì„ë² ë”© ìƒì„±
                    embedding = self.embedding_model.encode([chunk_text])[0].tolist()
                    
                    # ê³ ìœ  ID ìƒì„±
                    unique_id = f"{filename}_{chunk_id}"
                    
                    # ë²¡í„° DBì— ì €ì¥
                    self.collection.add(
                        embeddings=[embedding],
                        documents=[chunk_text],
                        metadatas=[{
                            "source": filename, 
                            "chunk_id": chunk_id,
                            "page_num": page_num,
                            "text_length": len(chunk_text)
                        }],
                        ids=[unique_id]
                    )
                    
                    # í˜ì´ì§€ ë§¤í•‘ ì €ì¥
                    self.page_mapping[unique_id] = {
                        'page_num': page_num,
                        'text': chunk_text,
                        'chunk_id': chunk_id
                    }
                    
                    progress_bar.progress((i + 1) / len(chunks))
                
                progress_bar.empty()
            
            # ì„¸ì…˜ ìƒíƒœì— í˜ì´ì§€ ë§¤í•‘ ì €ì¥
            st.session_state.page_mapping = self.page_mapping
            
            return True
            
        except Exception as e:
            st.error(f"ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return False
    
    def enhanced_search(self, query: str, n_results: int = 8) -> List[Dict]:
        """í–¥ìƒëœ ê²€ìƒ‰ - í˜ì´ì§€ ì •ë³´ì™€ ê´€ë ¨ì„± ì ìˆ˜ í¬í•¨"""
        if not self.collection:
            return []
        
        try:
            # ì§ˆë¬¸ì„ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜
            query_embedding = self.embedding_model.encode([query])[0].tolist()
            
            # ìœ ì‚¬í•œ ì²­í¬ ê²€ìƒ‰
            results = self.collection.query(
                query_embeddings=[query_embedding],
                n_results=n_results,
                include=['documents', 'metadatas', 'distances']
            )
            
            enhanced_results = []
            if results['documents']:
                for i, (doc, metadata, distance) in enumerate(zip(
                    results['documents'][0], 
                    results['metadatas'][0], 
                    results['distances'][0]
                )):
                    # ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚° (ê±°ë¦¬ë¥¼ ìœ ì‚¬ë„ë¡œ ë³€í™˜)
                    similarity_score = max(0, 1 - distance)
                    
                    enhanced_results.append({
                        'text': doc,
                        'page_num': metadata['page_num'],
                        'chunk_id': metadata['chunk_id'],
                        'similarity_score': similarity_score,
                        'rank': i + 1
                    })
            
            return enhanced_results
            
        except Exception as e:
            st.error(f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return []
    
    def generate_contextual_answer(self, question: str, search_results: List[Dict], api_key: str):
        """ë§¥ë½ì„ ê³ ë ¤í•œ ë‹µë³€ ìƒì„±"""
        if not search_results:
            return "ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", []
        
        try:
            genai.configure(api_key=api_key)
            model = genai.GenerativeModel('gemini-1.5-pro-latest')
            
            # í˜ì´ì§€ë³„ë¡œ ê²°ê³¼ ê·¸ë£¹í™”
            page_groups = {}
            for result in search_results:
                page_num = result['page_num']
                if page_num not in page_groups:
                    page_groups[page_num] = []
                page_groups[page_num].append(result)
            
            # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± - í˜ì´ì§€ ìˆœì„œëŒ€ë¡œ ì •ë ¬
            context_parts = []
            source_info = []
            
            for page_num in sorted(page_groups.keys()):
                page_results = page_groups[page_num]
                # ê° í˜ì´ì§€ì—ì„œ ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ì²­í¬ë“¤ ì„ íƒ
                page_results.sort(key=lambda x: x['similarity_score'], reverse=True)
                
                for i, result in enumerate(page_results[:2]):  # í˜ì´ì§€ë‹¹ ìµœëŒ€ 2ê°œ ì²­í¬
                    context_parts.append(f"[í˜ì´ì§€ {page_num}, ë°œì·Œ {len(context_parts)+1}]\n{result['text']}")
                    source_info.append({
                        'page_num': page_num,
                        'text': result['text'],
                        'similarity_score': result['similarity_score'],
                        'excerpt_num': len(context_parts)
                    })
            
            context = "\n\n".join(context_parts)
            
            prompt = f"""
ë‹¤ìŒì€ PDF ë¬¸ì„œì—ì„œ ë°œì·Œí•œ ë‚´ìš©ë“¤ì…ë‹ˆë‹¤:

{context}

ì§ˆë¬¸: {question}

ì§€ì‹œì‚¬í•­:
1. ìœ„ ë¬¸ì„œ ë°œì·Œ ë‚´ìš©ì—ì„œë§Œ ë‹µë³€í•˜ì„¸ìš”.
2. ë‹µë³€ì€ ë°˜ë“œì‹œ ì›ë¬¸ ê·¸ëŒ€ë¡œ ì¸ìš©í•˜ì„¸ìš”. ì ˆëŒ€ ìš”ì•½í•˜ê±°ë‚˜ ë°”ê¿”ì“°ì§€ ë§ˆì„¸ìš”.
3. ì—¬ëŸ¬ ë°œì·Œ ë‚´ìš©ì´ ê´€ë ¨ìˆë‹¤ë©´ ëª¨ë‘ í¬í•¨í•˜ì„¸ìš”.
4. ë¬¸ì„œì— í•´ë‹¹ ì •ë³´ê°€ ì—†ìœ¼ë©´ "ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë‹µí•˜ì„¸ìš”.
5. ë‹µë³€í•  ë•Œ ë°˜ë“œì‹œ ì¶œì²˜ë¥¼ ëª…ì‹œí•˜ì„¸ìš”: [í˜ì´ì§€ X, ë°œì·Œ Y]
6. ì§ˆë¬¸ê³¼ ì§ì ‘ ê´€ë ¨ëœ í•µì‹¬ ì •ë³´ë¥¼ ë¹ ëœ¨ë¦¬ì§€ ë§ˆì„¸ìš”.
7. ì›ë¬¸ì˜ ìˆ«ì, ë‚ ì§œ, ê³ ìœ ëª…ì‚¬ëŠ” ì •í™•íˆ ê·¸ëŒ€ë¡œ ì¸ìš©í•˜ì„¸ìš”.
8. ê° ë°œì·Œ ë‚´ìš©ì´ ì–´ëŠ í˜ì´ì§€ì—ì„œ ë‚˜ì˜¨ ê²ƒì¸ì§€ ëª…í™•íˆ í‘œì‹œí•˜ì„¸ìš”.
"""
            
            response = model.generate_content(prompt)
            return response.text, source_info
            
        except Exception as e:
            return f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}", []

# ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜
def main():
    st.title("ğŸ“„ PDF ë¬¸ì„œ ë¶„ì„ AI (ê³ ë„í™” ë²„ì „)")
    st.markdown("PDF ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ê³  ìì—°ì–´ë¡œ ì§ˆë¬¸í•˜ì„¸ìš”! ì •í™•í•œ í˜ì´ì§€ ì¶œì²˜ì™€ í•¨ê»˜ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤.")
    
    # ì‚¬ì´ë“œë°” - API í‚¤ ì„¤ì •
    with st.sidebar:
        st.header("âš™ï¸ ì„¤ì •")
        api_key = st.text_input("Google Gemini API í‚¤", type="password", 
                               help="https://aistudio.google.com/app/apikey ì—ì„œ ë°œê¸‰ë°›ìœ¼ì„¸ìš”")
        
        if not api_key:
            st.warning("API í‚¤ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”!")
        
        st.markdown("---")
        st.markdown("### ğŸ“– ì‚¬ìš©ë²•")
        st.markdown("""
        1. Google Gemini API í‚¤ ì…ë ¥
        2. PDF íŒŒì¼ ì—…ë¡œë“œ
        3. ë¬¸ì„œ ì²˜ë¦¬ ëŒ€ê¸°
        4. ì§ˆë¬¸ ì…ë ¥í•˜ì—¬ ê²€ìƒ‰
        """)
        
        st.markdown("---")
        st.markdown("### ğŸ¯ ê°œì„ ì‚¬í•­")
        st.markdown("""
        - âœ… ì •í™•í•œ í˜ì´ì§€ ì¶œì²˜ í‘œì‹œ
        - âœ… ë§¥ë½ ê³ ë ¤í•œ ì²­í‚¹
        - âœ… ê´€ë ¨ì„± ì ìˆ˜ í‘œì‹œ
        - âœ… í˜ì´ì§€ë³„ ê·¸ë£¹í™”
        """)
    
    # PDF ë¶„ì„ê¸° ì´ˆê¸°í™”
    if 'analyzer' not in st.session_state:
        st.session_state.analyzer = PDFAnalyzer()
    
    analyzer = st.session_state.analyzer
    
    # ëª¨ë¸ ì´ˆê¸°í™”
    if not st.session_state.get('models_initialized', False):
        if analyzer.initialize_models():
            st.session_state.models_initialized = True
            st.session_state.embedding_model = analyzer.embedding_model
            st.session_state.vectordb = analyzer.vectordb
    
    # íŒŒì¼ ì—…ë¡œë“œ
    uploaded_file = st.file_uploader("PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”", type=['pdf'])
    
    if uploaded_file is not None:
        col1, col2 = st.columns([2, 1])
        
        with col1:
            st.info(f"ğŸ“ ì—…ë¡œë“œëœ íŒŒì¼: {uploaded_file.name}")
        
        with col2:
            process_button = st.button("ğŸš€ ë¬¸ì„œ ì²˜ë¦¬ ì‹œì‘", type="primary")
        
        # ë¬¸ì„œ ì²˜ë¦¬
        if process_button and api_key:
            with st.spinner("PDF ë¬¸ì„œë¥¼ ê³ ë„í™”ëœ ë°©ì‹ìœ¼ë¡œ ì²˜ë¦¬í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                # í˜ì´ì§€ë³„ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                st.write("ğŸ” í˜ì´ì§€ë³„ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘...")
                pages_text = analyzer.extract_text_from_pdf(uploaded_file)
                
                if pages_text:
                    total_chars = sum(len(page['text']) for page in pages_text)
                    st.success(f"âœ… í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ ({len(pages_text)}í˜ì´ì§€, ì´ {total_chars:,} ê¸€ì)")
                    
                    # ìŠ¤ë§ˆíŠ¸ ì²­í¬ ë¶„í• 
                    st.write("ğŸ§  ìŠ¤ë§ˆíŠ¸ í…ìŠ¤íŠ¸ ë¶„í•  ì¤‘...")
                    chunks = analyzer.smart_text_chunking(pages_text)
                    st.success(f"âœ… í…ìŠ¤íŠ¸ ë¶„í•  ì™„ë£Œ ({len(chunks)}ê°œ ì²­í¬)")
                    
                    # í˜ì´ì§€ë³„ ì²­í¬ ë¶„í¬ í‘œì‹œ
                    page_distribution = {}
                    for chunk in chunks:
                        page_num = chunk['page_num']
                        page_distribution[page_num] = page_distribution.get(page_num, 0) + 1
                    
                    st.info(f"ğŸ“Š í˜ì´ì§€ë³„ ì²­í¬ ë¶„í¬: {dict(sorted(page_distribution.items()))}")
                    
                    # ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ìƒì„±
                    st.write("ğŸ—„ï¸ í–¥ìƒëœ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ìƒì„± ì¤‘...")
                    if analyzer.create_vector_database(chunks, uploaded_file.name):
                        st.success("ğŸ‰ ë¬¸ì„œ ì²˜ë¦¬ ì™„ë£Œ! ì´ì œ ì •í™•í•œ ì¶œì²˜ì™€ í•¨ê»˜ ì§ˆë¬¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                        st.session_state.documents_loaded = True
                        st.session_state.collection = analyzer.collection
                    else:
                        st.error("âŒ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ìƒì„± ì‹¤íŒ¨")
                else:
                    st.error("âŒ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨")
    
    # ì§ˆë¬¸ ì„¹ì…˜
    if st.session_state.documents_loaded and api_key:
        st.markdown("---")
        st.header("ğŸ’¬ ë¬¸ì„œì— ì§ˆë¬¸í•˜ê¸°")
        
        # ì˜ˆì‹œ ì§ˆë¬¸ë“¤
        st.markdown("**ğŸ’¡ ì§ˆë¬¸ ì˜ˆì‹œ:**")
        example_questions = [
            "ì˜¬í•´ íƒœì–‘ê´‘ íˆ¬ì ê³„íšì€ ì–´ë–»ê²Œ ë¼?",
            "ì£¼ìš” ì¬ë¬´ ì§€í‘œëŠ” ë¬´ì—‡ì¸ê°€?",
            "ë¦¬ìŠ¤í¬ ìš”ì¸ì€ ë¬´ì—‡ì¸ê°€?",
            "í–¥í›„ ì „ë§ì€ ì–´ë–¤ê°€?",
            "ë§¤ì¶œ ëª©í‘œëŠ” ì–¼ë§ˆì¸ê°€?",
            "ì£¼ìš” ì‚¬ì—… ë¶„ì•¼ëŠ”?"
        ]
        
        cols = st.columns(3)
        for i, question in enumerate(example_questions):
            with cols[i % 3]:
                if st.button(question, key=f"example_{i}"):
                    st.session_state.current_question = question
        
        # ì§ˆë¬¸ ì…ë ¥
        question = st.text_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:", 
                                value=st.session_state.get('current_question', ''),
                                placeholder="ì˜ˆ: ì˜¬í•´ ë§¤ì¶œ ëª©í‘œëŠ” ì–¼ë§ˆì¸ê°€ìš”?")
        
        col1, col2 = st.columns([3, 1])
        with col1:
            search_button = st.button("ğŸ” ì •ë°€ ê²€ìƒ‰", type="primary")
        with col2:
            clear_button = st.button("ğŸ—‘ï¸ ì´ˆê¸°í™”")
        
        if clear_button:
            st.session_state.current_question = ""
            st.rerun()
        
        if search_button and question:
            with st.spinner("ê´€ë ¨ ë‚´ìš©ì„ ì •ë°€ ê²€ìƒ‰í•˜ê³  ë§¥ë½ì„ ê³ ë ¤í•œ ë‹µë³€ì„ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                # í–¥ìƒëœ ê²€ìƒ‰
                search_results = analyzer.enhanced_search(question, n_results=8)
                
                if search_results:
                    # ë§¥ë½ì  ë‹µë³€ ìƒì„±
                    answer, source_info = analyzer.generate_contextual_answer(question, search_results, api_key)
                    
                    # ê²°ê³¼ í‘œì‹œ
                    st.markdown("### ğŸ“‹ ë‹µë³€")
                    st.markdown(answer)
                    
                    # ì¶œì²˜ ì •ë³´ í‘œì‹œ
                    if source_info:
                        st.markdown("### ğŸ“ ì¶œì²˜ ì •ë³´")
                        source_df_data = []
                        for info in source_info:
                            source_df_data.append({
                                "ë°œì·Œ ë²ˆí˜¸": info['excerpt_num'],
                                "í˜ì´ì§€": info['page_num'],
                                "ê´€ë ¨ì„±": f"{info['similarity_score']:.2%}",
                                "í…ìŠ¤íŠ¸ ê¸¸ì´": f"{len(info['text'])}ì"
                            })
                        
                        if source_df_data:
                            st.dataframe(source_df_data, use_container_width=True)
                    
                    # ìƒì„¸ ì°¸ì¡° ë‚´ìš©
                    with st.expander("ğŸ“š ìƒì„¸ ì°¸ì¡° ë‚´ìš© ë³´ê¸°"):
                        for i, result in enumerate(search_results[:6]):
                            st.markdown(f"**ğŸ“„ í˜ì´ì§€ {result['page_num']} | ê´€ë ¨ì„±: {result['similarity_score']:.2%} | ìˆœìœ„: {result['rank']}**")
                            st.text_area(
                                f"ë‚´ìš© {i+1}", 
                                result['text'], 
                                height=120, 
                                key=f"detail_chunk_{i}"
                            )
                            st.markdown("---")
                else:
                    st.warning("âš ï¸ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    elif not api_key:
        st.warning("âš ï¸ Google Gemini API í‚¤ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    elif not st.session_state.documents_loaded:
        st.info("ğŸ“„ PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê³  ì²˜ë¦¬í•´ì£¼ì„¸ìš”.")
    
    # í•˜ë‹¨ ì •ë³´
    st.markdown("---")
    st.markdown("**ğŸ”§ ê³ ë„í™” ê¸°ëŠ¥:** ì •í™•í•œ í˜ì´ì§€ ì¶œì²˜ í‘œì‹œ, ë§¥ë½ ê³ ë ¤ ì²­í‚¹, ê´€ë ¨ì„± ì ìˆ˜, í˜ì´ì§€ë³„ ê·¸ë£¹í™”")
    st.markdown("**ğŸ’¡ ì£¼ì˜ì‚¬í•­:** í˜„ì¬ í…ìŠ¤íŠ¸ë§Œ ë¶„ì„í•©ë‹ˆë‹¤. ì´ë¯¸ì§€, í‘œ, ê·¸ë˜í”„ ë¶„ì„ ê¸°ëŠ¥ì€ ì¶”í›„ ì¶”ê°€ ì˜ˆì •ì…ë‹ˆë‹¤.")

if __name__ == "__main__":
    main()
